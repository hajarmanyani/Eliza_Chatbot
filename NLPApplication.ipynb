{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fac5d2d",
   "metadata": {},
   "source": [
    "# Eliza Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99a2528",
   "metadata": {},
   "source": [
    "## Les regex dans le NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51f28b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates trouvées : ['12-05-1982', '03/04/1990', '23 Septembre 2010']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "texte = \"\"\"\n",
    "Paul est né le 12-05-1982 et Marie le 03/04/1990. Ils se sont rencontrés le 23 Septembre 2010.\n",
    "\"\"\"\n",
    "\n",
    "# Regex pour identifier les dates\n",
    "pattern = r'\\b(?:\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{2}-\\d{4}|\\d{1,2} [a-zA-Z]+ \\d{4})\\b'\n",
    "\n",
    "# Recherche des dates dans le texte\n",
    "dates = re.findall(pattern, texte)\n",
    "\n",
    "print(\"Dates trouvées :\", dates)\n",
    "\n",
    "# Résultat à obtenir: Dates trouvées : ['12-05-1982', '03/04/1990', '23 Septembre 2010']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca34e72",
   "metadata": {},
   "source": [
    "## Limites des Regex en NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f4ceb17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates trouvées : ['19 novembre 2023']\n"
     ]
    }
   ],
   "source": [
    "texte = \"\"\"\n",
    "Ce dimanche 19 novembre 2023 (19-11-23) marque une date historique pour T1, qui s'impose majestueusement avec un score de 3-0 contre Weibo Gaming.\n",
    "Faker était en 06-01-09.\n",
    "\"\"\"\n",
    "\n",
    "# Regex pour identifier les dates\n",
    "pattern = r'\\b(?:\\d{2}/\\d{2}/\\d{4}|\\d{2}-\\d{2}-\\d{4}|\\d{1,2} [a-zA-Z]+ \\d{4})\\b'\n",
    "\n",
    "# Recherche des dates dans le texte\n",
    "dates = re.findall(pattern, texte)\n",
    "\n",
    "print(\"Dates trouvées :\", dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa073b1",
   "metadata": {},
   "source": [
    "## ELIZA Chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a59d12",
   "metadata": {},
   "source": [
    "Pour créer un chatbot simple similaire à ELIZA basé sur les regex et les exemples fournis, nous allons écrire un script Python qui utilise des expressions régulières pour reconnaître des motifs spécifiques dans les entrées de l'utilisateur et y répondre de manière appropriée. Nous allons utiliser les patrons de réponse que vous avez fournis pour définir les règles du chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8187921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: It feels like everyone is ignoring me\n",
      "ELIZA: Tell me more about why you feel everyone is ignoring me.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def eliza_chatbot(user_input):\n",
    "    # TODO: Implementez vos règles pour identifier les patterns\n",
    "    # Exemple si \"always\" est detecté, vous pouvez retourner: \"CAN YOU THINK OF A SPECIFIC EXAMPLE\"\n",
    "    patterns_responses = [\n",
    "        (r\"I am (.*)\", [\"Why do you think you are {}?\", \"Can you tell me more about being {}?\"]),\n",
    "        (r\"It feels like everyone is (.*)\", [\"Tell me more about why you feel everyone is {}.\"]),\n",
    "        (r\"My (.*) is always (.*)\", [\"Why do you think your {} is always {}?\"]),\n",
    "        (r\"I think my (.*) doesn't (.*)\", [\"Why do you think your {} doesn't {}?\"]),\n",
    "        (r\"I need (.*)\", [\"Why do you need {}?\", \"Can you elaborate on why you need {}?\"]),\n",
    "        (r\"My (.*) never (.*)\", [\"Why do you think your {} never {}?\"])\n",
    "    ]\n",
    "    # Parcourez les motifs et vérifiez s'il y a une correspondance\n",
    "    for pattern, responses in patterns_responses:\n",
    "        match = re.match(pattern, user_input, re.IGNORECASE)\n",
    "        if match:\n",
    "            response = responses[0]  # Choisissez la première réponse pour la simplicité\n",
    "            return response.format(*match.groups())\n",
    "\n",
    "    return \"Tell me more about that.\"\n",
    "\n",
    "# Exemple d'utilisation\n",
    "user_input = input(\"You: \")\n",
    "print(\"ELIZA: \" + eliza_chatbot(user_input))\n",
    "\n",
    "# Testez:\n",
    "# I am sad today.\n",
    "# It feels like everyone is always ignoring me.\n",
    "# My boyfriend is always late.\n",
    "# I think my father doesn't understand me.\n",
    "# I need more help.\n",
    "# My sister nerver supports me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca0c140",
   "metadata": {},
   "source": [
    "# Concepts Clés en Traitement Automatique du Langage Naturel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d8092f",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a56e374",
   "metadata": {},
   "source": [
    "La tokenisation est le processus de découpage d'un texte en morceaux, appelés tokens. Ces tokens sont souvent des mots, mais peuvent aussi inclure des ponctuations et d'autres éléments. La tokenisation est une étape fondamentale en TALN car elle permet de préparer le texte pour des analyses plus approfondies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f545e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddd7873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import nltk\n",
    "# Données pour la Tokenisation\n",
    "nltk.download('punkt')\n",
    "# Données pour le pos_tagging\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5e56a",
   "metadata": {},
   "source": [
    "### Essayer de tokeniser une phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd96f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#', 'animals']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs. #animals\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Affichage des tokens\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a422b",
   "metadata": {},
   "source": [
    "### Utilisez le TweetTokenizer de nltk pour voir la différence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356dadb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'foxes', 'were', 'jumping', 'over', 'the', 'lazy', 'dogs', '.', '#animals']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs. #animals\"\n",
    "\n",
    "# Créer une instance de TweetTokenizer\n",
    "tweet_tokenizer = TweetTokenizer()\n",
    "\n",
    "# Tokenisation du texte\n",
    "tweet_tokens = tweet_tokenizer.tokenize(texte)\n",
    "\n",
    "print(tweet_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1deb920",
   "metadata": {},
   "source": [
    "## POS-tagging (Part-of-Speech Tagging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffbc6f8",
   "metadata": {},
   "source": [
    "Le POS-tagging est le processus d'assignation de tags de parties du discours (comme nom, verbe, adjectif, etc.) à chaque token d'un texte. Cela est utile pour comprendre la structure grammaticale d'une phrase et pour d'autres analyses syntaxiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7c32e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('were', 'VBD'), ('jumping', 'VBG'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dogs', 'NNS'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# POS-tagging des tokens\n",
    "pos_tags = pos_tag(tokens)\n",
    "\n",
    "# Affichage des résultats\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bb0975",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24ce6ae",
   "metadata": {},
   "source": [
    "Explorez la racinisation (Porter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "397a91af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'were', 'jump', 'over', 'the', 'lazi', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Initialisation du PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Application de la racinisation sur chaque token\n",
    "stems = [porter_stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(stems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8055cc5",
   "metadata": {},
   "source": [
    "## Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4b38df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a09ca86",
   "metadata": {},
   "source": [
    "Explorez la Lemmatisation et la différence avec le Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "31d96005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'were', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Initialisation du WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Application de la lemmatisation sur chaque token\n",
    "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db38ad7",
   "metadata": {},
   "source": [
    "Améliorez la Lemmatisation avec les POS-tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb974f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'be', 'jumping', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "texte = \"The quick brown foxes were jumping over the lazy dogs.\"\n",
    "\n",
    "# Tokenisation du texte\n",
    "tokens = word_tokenize(texte)\n",
    "\n",
    "# Initialisation du WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatisation en tenant compte du POS-tagging\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "# Affichage des résultats\n",
    "print(lemmatized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
